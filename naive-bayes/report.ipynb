{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief overview of the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we are expected to solve some theory questions about MLE and Naive Bayes in the first part. For the second part of the assignment, we are expected to decide whether a customer review is positive or negative by using sentiment analysis with Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Theory Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question, we have N samples $x_1, x_2.....x_N$ and we have a normal distribution with a certain mean and variance. However, both mean and the variance are unknown, and wa want to estimate them on the basis the observations. \n",
    "\n",
    "The first step is to write down the likelihood function that is the probability density function for the vector of observations given some set of parameters. Because of the independence, the joint distribution of the vector X's that we have obtained is the product of the PDF's of the individual X's of the $X_i$'s. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"likelihood.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to maximize this function. It is actually easier to maximize the logarithm of this expression and this is the same as minimizing the negative of the logarithm of this expression. We have a product. So, when we take the logarithm of this expression, we will get a sum of logarithms. The first term below appears when we take the logarithm of $\\frac{1}{\\sqrt{2 \\pi v}}$ and this happens n times because we have a product of n terms. The second term below appears when we take the logarithm of the $\\left( -\\frac{(x_i - \\mu )^2}{2 v} \\right)$ expression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to carry out the minimization, we need to take the derivative of the expression above with respect to $\\mu$, set it to zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"mean.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, Maximum Likelihood estimate of the mean takes very natural form. It is just the sample mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other probability, $p(x_{i} = 0 | c)$ is given by the normalisation requirement, $p(x_{i} = 0 | c) = 1 - p(x_{i} = 1 | c) = 1 - \\theta _ {i} ^ {c}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Naive Bayes conditional independence assumption the probability of observing a vector $x$ can be written as $p (\\mathbf{x} | c) = \\prod_{i = 1} ^ {D} p\\left(x_{i}|c\\right) = \\prod_{i = 1} ^ {D} \\left(\\theta_{i} ^ {c} \\right) ^ {x_{i}} \\left( 1 - \\theta_{i} ^ {c}\\right) ^ {1-x_{i}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, $x_{i}$ is either 0 or 1 and so each $i$ term contributes a factor $\\theta_{i} ^ {c}$ if $x_{i} = 1$ or $1 - \\theta_{i} ^ {c}$ if $x_{i} = 0.$ Together with the assumption that the training data is i.i.d. generated, the log likelihood of the attributes and class labels is $\\begin{array} {r l} {L} & {= \\sum_{n} \\log p \\left(\\mathbf{x} ^ {n} , c ^ {n} \\right) = \\sum_{n} \\log p \\left(c ^ {n} \\right) \\prod_{i} p\\left(x_{i} ^ {n} | c ^ {n} \\right)} \\\\\\\\ { } & { = \\sum_{i,n} x_{i} ^ {n} \\log \\theta_{i} ^ {c ^ {n}} + \\left(1 - x_{i} ^ {n} \\right) \\log \\left(1 - \\theta_{i} ^ {c ^ {n}} \\right) + n_{0} \\log p (c = 0) + n_{1} \\log p (c = 1)} \\end{array}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find the Maximum Likelihood optimal $\\theta_{i} ^ {c}$ by differentiating $ w.r.t. \\theta_{i} ^ {c}$ and equate to zero, that gives us $\\begin{aligned} \\theta_{i} ^ {c} = p\\left(x_{i} = 1 | c \\right) & = \\frac {\\sum _ { n } \\mathbb { I } \\left[ x _ { i } ^ { n } = 1 , c ^ { n } = c \\right] } { \\sum _ { n } \\mathbb { I } \\left[ x _ { i } ^ { n } = 0 , c ^ { n } = c \\right] + \\mathbb { I } \\left[ x _ { i } ^ { n } = 1 , c ^ { n } = c \\right] } \\\\\\\\ & = \\frac { \\text { number of times } x _ { i } = 1 \\text { for class } c } {\\text { number of datapoints in class } c } \\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p ( c ) = \\frac { \\text { number of times class c occurs } } { \\text { total number of data points } }$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the sample is (3,0,2,1,3,2,1,0,2,1), the likelihood term can be calculated like this\n",
    "$$\\begin{aligned}L(\\theta ) = & P ( X = 3 ) P ( X = 0 ) P ( X = 2 ) P ( X = 1 ) P ( X = 3 ) \\times P ( X = 2 ) P ( X = 1 ) P ( X = 0 ) P ( X = 2 ) P ( X = 1 ) \\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we substitute from the probability distribution given above, we wil get\n",
    "$$L (\\theta) = \\prod_{i = 1} ^ {n} P\\left(X_{i} | \\theta \\right) = \\left(\\frac { 2 \\theta } { 3 } \\right) ^ { 2 } \\left( \\frac {\\theta } { 3 } \\right) ^ { 3 } \\left(\\frac { 2 ( 1 - \\theta ) } { 3 } \\right) ^ { 3 } \\left(\\frac { 1 - \\theta } { 3 } \\right) ^ { 2 }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to maximize the likelihood function $L(θ)$, we can look at the log likelihood function\n",
    "$$\\begin{array} { r l } { l (\\theta ) } & { = \\log L(\\theta ) = \\sum_{i = 1 } ^ { n } \\log P \\left(X_{i} | \\theta \\right)} \\\\\\\\ { } & { = 2 \\left( \\log \\frac { 2 } { 3 } + \\log \\theta \\right) + 3 \\left(\\log \\frac { 1 } { 3 } + \\log \\theta \\right) + 3 \\left(\\log \\frac { 2 } { 3 } + \\log ( 1 - \\theta ) \\right) + 2 \\left(\\log \\frac { 1 } { 3 } + \\log ( 1 - \\theta ) \\right) } \\\\\\\\ { } & { = C + 5 \\log \\theta + 5 \\log ( 1 - \\theta ) } \\end{array}$$\n",
    "where $C$ is a constant that does not depend on $\\theta$ . We can see that the log likelihood function is easier to maximize compared to the likelihood function.\n",
    "Let the derivative of $l(\\theta)$ with respect to $\\theta$ be zero:\n",
    "$$\\frac{d l (\\theta)} {d \\theta} = \\frac {5} {\\theta} - \\frac{5} {1 - \\theta} = 0$$ \n",
    "and the solution gives us the MLE, which is $\\hat {\\theta } = 0.5 .$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Answer 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attrubutes are (Color, Type, Origin) <br>\n",
    "$P(Stolen = Yes)$ = 1/2 <br>\n",
    "$P(Stolen = No)$ = 1/2 <br>\n",
    "$P(Color=Red|Stolen=Yes)$ = 3/5         $\\;\\;\\;\\;\\;\\;$ $\\;\\;\\;\\;\\;\\;$ $P(Color = Red|Stolen=No)$ = 2/5 <br>\n",
    "$P(Type = SUV|Stolen = Yes)$ = 2/5      $\\;\\;\\;\\;\\;\\;$$\\;\\;\\;\\;\\;\\;$ $P(Type = SUV|Stolen = No)$ = 2/5 <br>\n",
    "$P(Origin=Domestic | Stolen=Yes)$ = 3/5 $\\;\\;\\;\\;\\;$$P(Origin=Domestic|Stolen=No)$ = 2/5 <br><br>\n",
    "$P(Stolen = Yes)$*$P(Color=Red, Type=SUV, Origin=Domestic | Stolen=Yes)$ = 1/2 * 3/5 * 2/5 * 3/5 = 9/125 = 0.072 <br>\n",
    "$P(Stolen = No)$*$P(Color=Red, Type=SUV, Origin=Domestic | Stolen=No)$ = 1/2 * 2/5 * 2/5 * 2/5 = 4/125 = 0.032   <br><br>\n",
    "Since 0.072 > 0.032, **Red Domestic SUV** is **Stolen**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Answer 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question vector has attributes x = (rich, married, healthy)<br>\n",
    "$P(rich=1|content=1)$ = 3/4<br>\n",
    "$P(rich=1|content=0)$ = 1/5<br>\n",
    "$P(married=1|content=1)$ = 2/4<br>\n",
    "$P(married=1|content=0)$ = 1/5<br>\n",
    "$P(healthy = 1|content=1)$ = 3/4<br>\n",
    "$P(healthy=1 |content = 0)$ = 1/5<br>\n",
    "$P(content=0)$ = 5/9<br>\n",
    "$P(content=1)$ = 4/9<br>\n",
    "$P(rich =0|content=1)$ = 1/4<br>\n",
    "$P(rich=0|content=0)$ = 4/5<br>\n",
    "$P(married=0|content=1)$ = 2/4<br>\n",
    "$P(married=0|content=0)$ = 4/5<br>\n",
    "$P(married=0|content=1)$ = 2/4<br>\n",
    "$P(helathy=0|content=1)$ = 1/4<br>\n",
    "$P(healthy=0|content=0)$ = 4/5<br>\n",
    " $\\;\\;\\;\\;\\;\\;$ **Using Naive Bayes, what is the probability that a person who is ’not rich’, ’married’ and ’healthy’ is ’content’?**<br>\n",
    " $P(content=1|rich=0, married=1, helathy=1)$ = (1/4 * 1/2 * 3/4 * 4/9) / (4/9 * 1/4 * 1/2 * 3/4 + 5/9 * 4/5 * 1/5 * 1/5) = 0.70\n",
    " \n",
    "  $\\;\\;\\;\\;\\;\\;$ **What is the probability that a person who is ’not rich’ and ’married’ is ’content’? (That is, we do not know whether ot not they are ’healthy’.)**<br>\n",
    " $P(content=1|rich=0,married=1,healthy=1)$ + $P(content=1|rich=0, married=1, healthy=0)$ = 0*70 + (1/4 * 1/2 * 1/4 * 4/9) / (4/9 * 1/4 * 2/4 * 1/4 + 5/9 * 4/5 * 1/5 * 4/5) = 0.70 + 0.1634 = 0.8634"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning 2nd Assignment Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Understanding the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we are expected to predict whether a review is positive or negative from words that appear in the review. It can be feasible with NaiveBayes classifier since it is known to outperform even highly sophisticated classification methods. It actually assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the dataset, there are $5914$ reviews that are labeled as negative, and $6000$ reviews that are labeled as positive. When I split the data as 80% for training and the remainder for test, I got approximately $4713$ negative and $4818$ positive samples. The most frequent words that appear in the positive case are $'great': 2030$, $'like': 1875$ and $'book': 1747$. The most frequent words that appear in the negative case are $'like': 2293$, $'just': 2105$ and $'book': 1942$. As we can see these statistics, '$like$' and '$book$' words are appear as most frequent words in both class. In fact, the most effective words should be the words with the greatest difference. For example, a less common word will have more impact on the positive class if it appears less in the negative class. When we look at the words with the greatest difference in frequency, we can see these words with their frequencies. \n",
    "    $'great': 1182$,\n",
    "    $'best': 577$,\n",
    "    $'love': 555$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implementing Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we are expected to implement our Naive Bayes classifier by using Bag of Words (BoW) model. My Naive Bayes classifier works with $sentiment$ $category$ as well as $topic$ $category$ label. I used Laplace Smoothing to increase the accuracy. When I use the $sentiment$ $category$ as training label, I got approximately %81 accuracy. However, when I use the $topic$ category\" as training label, I got approximately %91 accuracy because the words that are in a certain $topic$ $category$ are often found in other documents in the same category. This actually increases the conditional probability for these words in the specific category class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of Words Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the \"**all_sentiment_shuffled.txt**\" file, the 3rd column is labeled as the **document tokens** . Each tokens in a row data represents our documents and this text document is represented as a bag of words. That means text document is represented as an unordered set of words with their position ignored, keeping only their frequency in the document. <br>\n",
    "A Naive Bayes classifier may use all words in the text as features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Unigram features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While I was implementing my Naive Bayes Classifier, I have calculated sentiment category probabilities (neg,pos) or topic category probabilities (books, camera, dvd, health, music, software) seperately by using dictionary. I used CountVectorizer from sklearn library for feature extraction. There are  5914  reviews that are labeled as negative, and  6000  reviews that are labeled as positive. $P(negLabel) ≈ 0.4967$ and $P(posLabel) ≈ 0.5032$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I siplit the data into two parts as $training$ and $testing$ data, there are approximately $41888$ features for the training. After feature extraction and calculating the probability of sentiment category phase done, I have calculated the conditional probabilities for each word in the documents. In order to remember these conditional probabilities in the predicting phase, I have used dictionary again to store all of these probabilities seperately. <br>\n",
    "My Naive Bayes calculations is done in $log  space$, in order to avoid underflow and increase speed. In order to estimate the conditional probabilities $P(w_i|c)$ <br>\n",
    "$\\;\\;\\;\\;\\;\\;$**➢** I first filtered all documents with the sentiment category c into one dataframe <br>\n",
    "$\\;\\;\\;\\;\\;\\;$**➢** Then, I have used the frequency of $w_i$ in this filtered dataframe to give a maximum likelihood estimate of the probability.<br>\n",
    "$\\;\\;\\;\\;\\;\\;$**➢** The vocabularies consists of the union of all the word types in all classes, not just the words in one class c. <br>\n",
    "I have computed $P(w_i|positive)$ and $P(w_i|negative)$ as follows: <br>\n",
    "$P(w_i|positive) = \\frac { \\text { count(w_i,positive) } } { {\\sum _ {w∈V}count(w,positive) } }$\n",
    "$P(w_i|negative) = \\frac { \\text { count(w_i,negative) } } { {\\sum _ {w∈V}count(w,negative) } }$ <br><br>\n",
    "In order to get rid of zero probability values for $P(wi|c)$, I have used a Laplace smoothing method.\n",
    "$P(w_i|c) = \\frac { \\text { count(w_i,c)+1 } } { {\\sum _ {w∈V}(count(w,c)+1)} }$ = $\\frac { \\text { count(w_i,c)+1 } } { {(\\sum _ {w∈V}count(w,c)+|V|)} }$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some words that occur in my test data but are not in my vocabularies at all because they did not occur in any training document in any class. In this situation, I have removed them from the test document and not include any probability for them at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bigram features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Constrains on Data:** After extracting the bigram features, I have got approximately 420.000 features and when I convert it to dataframe, the calculations took very long time to get accuracy. Therefore, I reduce the size of the dataframe. When I was making a prediction in my new dataframe I got approximately %62 accuracy for 42031 features. This accuracy actually very good when it is considered that the calculations made with relatively small features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I follow the same rules that I have mentioned above. (for the Unigram features.) In this case the features are in the binary form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For error analysis case, I used Confusion Matrix to see where I predicted the reviews falsely. I used it for also predicting topic category label. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion Matrix (For Sentiment Category):**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"confusion.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, if we look at the confusion matrix, the number of false predicted values mostly occured in the positive review prediction. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\;\\;\\;\\;\\;\\;$**➢** For the negative labeled reviews, 186 of the 1201 reviews are predicted false. These are predicted as positive while they are negative labeled reviews. <br>\n",
    "$\\;\\;\\;\\;\\;\\;$**➢** For the positive labeled reviews, 250 of the 1182 reviews are predicted false. These are predicted as negative while they are positive labeled reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification Report:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the classification report, $negative$ predictions has given us the best result <br>\n",
    "$\\;\\;\\;\\;\\;\\;$**➢** For $negative$ labeled dataset, **%85** of all $negative$ datasets are predicted as true, and **%80** of the model's prediction about $negative$ labels are the real $negative$ reviews. <br>\n",
    "$\\;\\;\\;\\;\\;\\;$**➢** For $positive$ labeled dataset, **%79** of all $positive$ datasets are predicted as true, and **%83** of the model's prediction about $positive$ labels are the real $positive$ reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I observed that my model mostly confuse on predicting positive labels. This is because the conditional probability of the common most frequent words such as \"like\" and \"book\" are higher in the negative labeled reviews. So it makes the posterior term for the negative cases much higher than the positive cases' posterior term. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Category Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we can find out how frequent the categories in the corpus. <br>\n",
    "As we can see the picture below, the prababilities of topic categories are listed seperately. I have observed that the number of the different documents for each topic category label are approximately the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"topic.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion Matrix: (For Topic Category)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"conf_topic.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, if we look at the confusion matrix, the number of false predicted values mostly occured in the \"books\" and \"dvd\" category prediction. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\;\\;\\;\\;\\;\\;$**➢** For the \"books\" labeled category, 54 of the 401 category are predicted false. 3 data are predicted as \"camera\", 28 data are predicted as \"dvd\", 3 data are predicted as \"health\", 6 data are predicted as \"music\", and 14 data are predicted as \"software\" while they are in the \"books\" topic category label. <br><br>\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;$**➢** For the \"camera\" labeled category, 16 of the 412 category are predicted false. 1 data are predicted as \"books\", 1 data are predicted as \"dvd\", 8 data are predicted as \"health\", 0 data are predicted as \"music\", and 6 data are predicted as \"software\" while they are in the \"camera\" topic category label. <br><br>\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;$**➢** For the \"dvd\" labeled category, 54 of the 420 category are predicted false. 18 data are predicted as \"books\", 6 data are predicted as \"camera\", 6 data are predicted as \"health\", 20 data are predicted as \"music\", and 4 data are predicted as \"software\" while they are in the \"dvd\" topic category label. <br><br>\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;$**➢** For the \"health\" labeled category, 36 of the 401 category are predicted false. 4 data are predicted as \"books\", 19 data are predicted as \"camera\", 1 data are predicted as \"dvd\", 4 data are predicted as \"music\", and 8 data are predicted as \"software\" while they are in the \"health\" topic category label. <br>\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;$**➢** For the \"music\" labeled category, 22 of the 375 category are predicted false. 1 data are predicted as \"books\", 0 data are predicted as \"camera\", 10 data are predicted as \"dvd\", 3 data are predicted as \"health\", and 8 data are predicted as \"software\" while they are in the \"music\" topic category label. <br><br>\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;$**➢** For the \"software\" labeled category, 26 of the 374 category are predicted false. 9 data are predicted as \"books\", 6 data are predicted as \"camera\", 6 data are predicted as \"dvd\", 2 data are predicted as \"health\", and 3 data are predicted as \"music\" while they are in the \"software\" topic category label. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification Report:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the classification report, $health$ predictions has given us the best result <br>\n",
    "$\\;\\;\\;\\;\\;\\;$**➢** For $books$ labeled dataset, **%87** of all $health$ labeled documents are predicted as true, and **%91** of the model's prediction about $books$ labels are the real $books$ labeled data. <br><br>\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;$**➢** For $camera$ labeled dataset, **%96** of all $camera$ labeled documents are predicted as true, and **%92** of the model's prediction about $camera$ labels are the real $camera$ labeled data. <br><br>\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;$**➢** For $dvd$ labeled dataset, **%87** of all $dvd$ labeled documents are predicted as true, and **%89** of the model's prediction about $dvd$ labels are the real $dvd$ labeled data. <br><br>\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;$**➢** For $health$ labeled dataset, **%91** of all $health$ labeled documents are predicted as true, and **%94** of the model's prediction about $health$ labels are the real $health$ labeled data. <br><br>\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;$**➢** For $music$ labeled dataset, **%94** of all $music$ labeled documents are predicted as true, and **%91** of the model's prediction about $music$ labels are the real $music$ labeled data. <br><br>\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;$**➢** For $software$ labeled dataset, **%93** of all $software$ labeled documents are predicted as true, and **%90** of the model's prediction about $software$ labels are the real $software$ labeled data. <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I observed that my model mostly confuse on predicting \"books\" and \"dvd\" labels. This is because the conditional probability of the common most frequent words such as \"like\" and \"just\" are higher in the \"dvd\" labeled reviews. So it makes the posterior term for the \"dvd\" labeled category cases much higher than the \"books\" labeled category cases' posterior term. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Error Analysis For Bigram Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion Matrix:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"conf_bigram.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, if we look at the confusion matrix, the number of false predicted values mostly occured in the positive review prediction again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\;\\;\\;\\;\\;\\;$**➢** For the negative labeled reviews, 685 of the 1202 reviews are predicted false. These are predicted as positive while they are negative labeled reviews. <br>\n",
    "$\\;\\;\\;\\;\\;\\;$**➢** For the positive labeled reviews, 226 of the 1181 reviews are predicted false. These are predicted as negative while they are positive labeled reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification Report:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the classification report, $negative$ predictions has given us the best result <br>\n",
    "$\\;\\;\\;\\;\\;\\;$**➢** For $negative$ labeled dataset, **%43** of all $negative$ datasets are predicted as true, and **%70** of the model's prediction about $negative$ labels are the real $negative$ reviews. <br>\n",
    "$\\;\\;\\;\\;\\;\\;$**➢** For $positive$ labeled dataset, **%81** of all $positive$ datasets are predicted as true, and **%58** of the model's prediction about $positive$ labels are the real $positive$ reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Error Analysis of TF-IDF for Sentiment Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### By using TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion Matrix (For Sentiment Category):**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"tf-idf-sent.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, I have improved the accuracy result by using TF-IDF. With Tfidftransformer I have computed the word counts by using CountVectorizer and then compute the Inverse Document Frequency (IDF) values and only then compute the Tf-idf scores. (I have mentioned this in TF-IDF part below.) <br>\n",
    "By the help of the Tfidfvectorizer, I have done all three steps at once. Under the hood, it actually computes the word counts, IDF values, and Tf-idf scores all using the same dataset. The improved new accuracy for the sentiment classification is 0.84. (That was 0.81 by using CountVectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason why the accuracy result is increased is that selecting a subset of extremely effective words for the dictionary improved the model. In the BoW model, all the words in the documents have the same importance but this time I have calculated the Tf-idf scores of the words that appears in each document. This selection have improved also my accuracy while I was predicting the topic categories for each document. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Modul Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer of a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### For Unigram Features (With Stop Words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\;\\;\\;\\;\\;\\;$**➢** The words whose presence most strongly predicts that the review is positive can be chosen among the words with the highest probability of conditionals for positively labeled comments. In other words, 10 words that have the maximum $\"P(word|label=\"pos\")\"$ probability. These are <br>\n",
    "$P(word='the'|label = pos)$ = -2.951629226875015<br>\n",
    "$P(word='and'|label = pos)$ = -3.5324260221284973<br>\n",
    "$P(word='to'|label = pos)$ = -3.694511957464928<br>\n",
    "$P(word='of'|label = pos)$ = -3.8148526921097363<br>\n",
    "$P(word='it'|label = pos)$ = -3.984913963595256<br>\n",
    "$P(word='is'|label = pos)$ = -3.988409725351052<br>\n",
    "$P(word='this'|label = pos)$ = -4.226656509045062<br>\n",
    "$P(word='in'|label = pos)$ = -4.301815576186066<br>\n",
    "$P(word='that'|label = pos)$ = -4.5184658818056205<br>\n",
    "$P(word='for'|label = pos)$ = -4.568148912899209<br>\n",
    "with the given conditional probabilities result.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\;\\;\\;\\;\\;\\;$**➢** In order to list the 10 words whose absence most strongly predicts that the review is positive, we need to check the common words that have the highest possibility in the other class. These words are \"to\", \"it\", \"this\", \"that\", \"was\", \"not\", \"on\", \"but\", \"have\", \"they\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\;\\;\\;\\;\\;\\;$**➢** In order to list the 10 words whose presence most strongly predicts that the review is negative we need to find 10 words which have the highest possibility of $\"P(word|negative)\"$ probability. <br>\n",
    "$P(word='the'|label = neg)$ = -2.976663121086348<br>\n",
    "$P(word='to'|label = neg)$ = -3.636150281280282<br>\n",
    "$P(word='and'|label = neg)$ = -3.6768993070676066<br>\n",
    "$P(word='of'|label = neg)$ = -3.888916851725684<br>\n",
    "$P(word='it'|label = neg)$ = -3.918901932423729<br>\n",
    "$P(word='is'|label = neg)$ = -4.097065157685941<br>\n",
    "$P(word='this'|label = neg)$ = -4.180414572947828<br>\n",
    "$P(word='that'|label = neg)$ = -4.379086373278228<br>\n",
    "$P(word='in'|label = neg)$ = -4.409486122954875<br>\n",
    "$P(word='for'|label = neg)$ = -4.650498688329869<br>\n",
    " with the given conditional probability results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\;\\;\\;\\;\\;\\;$**➢** In order to list the 10 words whose absence most strongly predicts that the review is negative, we need to check the common words that have the highest possibility in the other class again. So these words are \"the\", \"and\", \"of\", \"is\", \"in\", \"for\", \"you\", \"with\", \"as\", \"my\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### For Unigram Features (Without Stop Words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\;\\;\\;\\;\\;\\;$**➢** The words whose presence most strongly predicts that the review is positive can be chosen among the words with the highest probability of conditionals for positively labeled comments. In other words, 10 words that have the maximum $\"P(word|label=\"pos\")\"$ probability. These are <br>\n",
    " $'great': -5.096467046299068$,<br>\n",
    " $'like': -5.146528571369659$,<br>\n",
    " $'just': -5.246722316948728$,<br>\n",
    " $'good': -5.278489694108583$,<br>\n",
    " $'book': -5.279685866500013$,<br>\n",
    " $'camera': -5.45307081332756$,<br>\n",
    " $'time': -5.473927376602117$,<br>\n",
    " $'use': -5.511695656675084$,<br>\n",
    " $'really': -5.621679000485739$,<br>\n",
    " $'does': -5.6815667413803626$,<br>\n",
    "with the given conditional probabilities result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\;\\;\\;\\;\\;\\;$**➢** In order to list the 10 words whose absence most strongly predicts that the review is positive, we need to check the common words that have the highest possibility in the other class. These words are \"like\", \"just\", \"good\", \"book\", \"camera\", \"time\", \"does\", \"movie\", \"film\", \"product\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\;\\;\\;\\;\\;\\;$**➢** In order to list the 10 words whose presence most strongly predicts that the review is negative we need to find 10 words which have the highest possibility of $\"P(word|negative)\"$ probability. <br>\n",
    " $'like': -4.973325357308957$, <br>\n",
    " $'just': -5.0409310036538395$,<br>\n",
    " $'book': -5.139697053586385$,<br>\n",
    " $'good': -5.393697850171875$,<br>\n",
    " $'camera': -5.442622522378365$,<br>\n",
    " $'did': -5.451845716529569$,<br>\n",
    " $'does': -5.45398628102868$,<br>\n",
    " $'time': -5.467650643222738$,<br>\n",
    " $'movie': -5.642038512531619$,<br>\n",
    " $'product': -5.791459017983477$<br>\n",
    " with the given conditional probability results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\;\\;\\;\\;\\;\\;$**➢** In order to list the 10 words whose absence most strongly predicts that the review is negative, we need to check the common words that have the highest possibility in the other class again. So these words are $\"great\"$, $\"use\"$, $\"really\"$, $\"new\"$, $\"album\"$, $\"cd\"$, $\"music\"$, $\"easy\"$, $\"read\"$, $\"years\"$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For Bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\;\\;\\;\\;\\;\\;$**➢** The words whose presence most strongly predicts that the review is positive can be chosen among the words with the highest probability of conditionals for positively labeled comments. In other words, 10 bigram words that have the maximum $\"P(word|label=\"pos\")\"$ probability. These are <br>\n",
    " 'battery life': -7.138,<br>\n",
    " 'big fan': -7.967,<br>\n",
    " '10 years': -8.010,<br>\n",
    " 'black white': -8.010,<br>\n",
    " 'aa batteries': -8.101,<br>\n",
    " 'absolutely love': -8.101,<br>\n",
    " 'adobe photoshop': -8.101,<br>\n",
    " '20th century': -8.15007822,<br>\n",
    " 'amazon com': -8.150078225,<br>\n",
    " 'best friend': -8.1500782253<br>\n",
    "with the given conditional probabilities result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\;\\;\\;\\;\\;\\;$**➢** In order to list the 10 bigram words whose absence most strongly predicts that the review is positive, we need to check the common words that have the highest possibility in the other class. These bigram words are:  **'album like','blood pressure', 'amazon com', '10 minutes', 'angle lens', '10 years', '15 minutes', 'anti virus', 'album just', 'bad guys'.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\;\\;\\;\\;\\;\\;$**➢** In order to list the 10 bigram words whose presence most strongly predicts that the review is negative we need to find 10 bigram words which have the highest possibility of $\"P(word|negative)\"$ probability. <br>\n",
    " 'album like': -7.3497,<br>\n",
    " 'blood pressure': -7.6682,<br>\n",
    " 'amazon com': -7.7666,<br>\n",
    " '10 minutes': -7.9150,<br>\n",
    " 'angle lens': -7.915099,<br>\n",
    " '10 years': -7.99848148,<br>\n",
    " '15 minutes': -7.998481,<br>\n",
    " 'anti virus': -8.089453,<br>\n",
    " 'big fan': -8.1895,<br>\n",
    " 'big mistake': -8.18953<br>\n",
    "  with the given conditional probability results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\;\\;\\;\\;\\;\\;$**➢** In order to list the 10 words whose absence most strongly predicts that the review is negative, we need to check the common words that have the highest possibility in the other class again. So these words are: **'battery life', 'big fan', 'black white', 'aa batteries', 'absolutely love', '20th century', 'best friend', 'big deal', '20 minutes', '20 years'.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, I narrowed down the dictionary by choosing specific words for positive or negative reviews. In order to improve my classification result, I used $TfidfVectorizer$ instead of $CountVectorizer$ for feature extraction. While I was obtaining the terms, I have computed tf-idf scores on each documents within my $training$ dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer of b (Stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of 10 non-stopwords that most strongly predict that the review is positive is listed below. <br>\n",
    " $'great': -6.260552971004138$,<br>\n",
    " $'book': -6.539143266080537$,<br>\n",
    " $'camera': -6.636673953570318$,<br>\n",
    " $'good': -6.648847185138457$,<br>\n",
    " $'use': -6.67715563732412$,<br>\n",
    " $'like': -6.74474181571239$,<br>\n",
    " $'love': -6.831652657546714$,<br>\n",
    " $'just': -6.849164188830696$,<br>\n",
    " $'easy': -6.8853927387975045$,<br>\n",
    " $'movie': -6.900785935772376$<br>\n",
    "These words are calculated by using conditional probability of $P(word|label=\"positive\")$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of 10 non-stopwords that most strongly predict that the review is negative is listed below.<br>\n",
    " 'book': -6.488679207189636,<br>\n",
    " 'camera': -6.518733025754922,<br>\n",
    " 'just': -6.586412453375094,<br>\n",
    " 'like': -6.593626392702126,<br>\n",
    " 'did': -6.748589253207188,<br>\n",
    " 'product': -6.801121323983279,<br>\n",
    " 'good': -6.821432334024917,<br>\n",
    " 'does': -6.825737659342416,<br>\n",
    " 'movie': -6.841883750614543,<br>\n",
    " 'time': -6.936284775849141<br>\n",
    " These words are calculated by using conditional probability of  $P(word|label=\"negative\")$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer of c (Analyzing effect of the stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why might it make sense to remove stop words when interpreting the model?**<br>\n",
    "$\\;\\;\\;\\;\\;\\;$**➢** It is make sense to remove stop words when they are the $\"meaningless\"$ words for the documents because it can cause false predictions in the corpus. However these words should be the common meaningless words for all documents. Otherwise we can loose a word that can be meaningful for the other documents.<br>\n",
    "For example, in the dataset, there are approximately same number of \"the\" and and \"to\" words. We can remove these words since these stop words has the same meaning. However if any stop word occur in the positive labeled dataset and also if the frequency of that word is very higher than the class of negative's one, we do not have to remove this word because we can assume that this word has a meaining itself for the positive labeled dataset. <br><br>\n",
    "**Why might it make sense to keep stopwords?** <br>\n",
    "$\\;\\;\\;\\;\\;\\;$**➢** Stop words are actually the words that has high frequency on a corpus. It might be considered good to get them all out of the corpus to make sure that the stop words won't cause any wrong or bad situation. Hovewer, there’s no universal stop words list. Depending on the problem we are analyzing, the word may be meaningless means that any word can be a stop word depending on what we are trying to do. If we remove all stop words from the corpus, we can loose a word that is valuable to some documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Using CountVectorizer in TF-IDF "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a sense of how the IDF values look, I have printed it by placing the IDF values in a python DataFrame. The values will be sorted in ascending order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"idf.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the picture above, the words $like$, $just$ and $good$ have the lowest IDF values. This is expected as these words appear in each and every document in my corpus. The lower the IDF value of a word, the less unique it is to any particular document. I also find these words in the \"Modul Analysis\" part when I have mentioned about calculating conditional probabilities. This \"idf-weigths\" result is actually proof of accuracy of my calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the picture below, I have printed the words that have highest tf-idf score for the first document in the traning dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"tf-idf-score.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### using TfidfVectorizer in TF-IDF "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, I used TF-IDF vectorizer to calculate the accuracy. With Tfidfvectorizer I have computed the word counts, idf and tf-idf values all at once. These values are identical to the ones from Tfidftransformer, the only difference is that it’s done in just two steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I got %92 accuracy in this case because I got specific words that have higher score (mentioned more about in the error analysis case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Calculation of Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I mentioned abode, I have implemented a Naive Bayes Classifier that works both for sentiment_category and topic_category labels. The obtained accuracies can be seen the picture above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"acc_unigram.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
