{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "df = pd.read_csv(\"head.csv\")\n",
    "clf_label = \"sentiment_category\" # can be changed with topic_category\n",
    "X = df.the_document_tokens\n",
    "if(clf_label == \"sentiment_category\"):\n",
    "    y = df.sentiment_category\n",
    "elif(clf_label == \"topic_category\"):\n",
    "    y = df.topic_category\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(type(X))\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.20)\n",
    "print(type(X_train))\n",
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer = CountVectorizer(stop_words='english')\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train) \n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42223"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names()\n",
    "len(feature_names)\n",
    "#len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train.toarray(), columns=feature_names) # converting Series to Dataframe\n",
    "X_test = pd.DataFrame(X_test.toarray(), columns=feature_names) # converting Series to Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[clf_label+\"_label\"] = np.array(y_train, dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9531 entries, 0 to 9530\n",
      "Columns: 42224 entries, 00 to sentiment_category_label\n",
      "dtypes: int64(42223), object(1)\n",
      "memory usage: 3.0+ GB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(X_train.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [category for category in X_train.sentiment_category_label.unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trains = {}\n",
    "for sent_category in categories:\n",
    "    X_trains[sent_category] = X_train[X_train['sentiment_category_label']==sent_category]\n",
    "    X_trains[sent_category] = X_trains[sent_category].iloc[: , :-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4723, 4808)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_trains[\"neg\"]), len(X_trains[\"pos\"]) # number of documents in negative and positive cases in training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_freq_in_negative_case = {} # created to store most freqeunt words in negative labeled documents\n",
    "for word in X_trains[\"neg\"]:\n",
    "    freq = X_trains[\"neg\"][word].sum() # sum of the specific word in negative case \n",
    "    max_freq_in_negative_case[word] = freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_freq_in_negative_case = dict(sorted(max_freq_in_negative_case.items(), key=lambda item: item[1], reverse=True))\n",
    "count = 1\n",
    "for key in max_freq_in_negative_case:\n",
    "    print(key,\"->freq: \",max_freq_in_negative_case[key])\n",
    "    if(count == 10): break\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_freq_in_positive_case = {} # created to store most freqeunt words in positive labeled documents\n",
    "for word in X_trains[\"pos\"]:\n",
    "    freq = X_trains[\"pos\"][word].sum() # sum of the specific word in positive case \n",
    "    max_freq_in_positive_case[word] = freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_freq_in_positive_case = dict(sorted(max_freq_in_positive_case.items(), key=lambda item: item[1], reverse=True)) \n",
    "count = 1\n",
    "for key in max_freq_in_positive_case:\n",
    "    print(key,\"->freq: \",max_freq_in_positive_case[key])\n",
    "    if(count == 10): break\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = {} # to store freqeuency difference of common words in positive and negative cases\n",
    "for key in max_freq_in_negative_case.keys(): \n",
    "    difference[key] = abs(max_freq_in_positive_case[key] - max_freq_in_negative_case[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and ->freq:  2460\n",
      "not ->freq:  1922\n",
      "was ->freq:  1397\n",
      "is ->freq:  1313\n",
      "great ->freq:  1153\n",
      "of ->freq:  1133\n",
      "that ->freq:  988\n",
      "the ->freq:  980\n",
      "to ->freq:  838\n",
      "as ->freq:  818\n"
     ]
    }
   ],
   "source": [
    "difference = dict(sorted(difference.items(), key=lambda item: item[1], reverse=True))\n",
    "count = 1\n",
    "for key in difference:\n",
    "    print(key,\"->freq: \",difference[key])\n",
    "    if(count == 10): break\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train, dtype='object')\n",
    "y_test = np.array(y_test, dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['books' 'dvd' 'health' ... 'health' 'dvd' 'software']\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes Classifier (Works for both sentiment category and topic category)\n",
    "class NaiveBayesClassifier: \n",
    "    # outcomesPD is X, y is y_train -> dtype = 'object'\n",
    "    def fit(self, X, y_train, clf_label):\n",
    "        (unique, counts) = np.unique(y_train, return_counts=True) # count the unique number of classes in the category label\n",
    "        frequencies = np.asarray((unique, counts)).T \n",
    "        self.freq = {} # to store the frequency of the categories seperately\n",
    "        for category, frequency in frequencies:\n",
    "            self.freq[category] = frequency  # store the freqeuncy for each category\n",
    "        total_examples = sum(self.freq.values()) # total number of documents in the dataset\n",
    "        prob_category = {} # to store the category probabilities\n",
    "        for category in self.freq.keys():\n",
    "            prob_category[category] = self.freq[category] / total_examples # to calculate the probability of the category class\n",
    "        print(prob_category) # print the probabilities of each category seperately\n",
    "        number_of_word_in_category_case = {} # to store the number of word in the specific category\n",
    "        outcomesCategoryPD = {} # will be used in filtering the categories from the training data\n",
    "        prob_word_given_category = {} # category, word\n",
    "        for category in self.freq.keys(): \n",
    "            outcomesCategoryPD[category] = X[X[clf_label]==category] # filtering operation\n",
    "            number_of_word_in_category_case[category] = outcomesCategoryPD[category].iloc[: , :-1].values.sum() # total number of word is specific category case\n",
    "            prob_word_given_category[category] = {} # to store the conditional probability of each word for each class\n",
    "        vocabularies = [word for word in X.iloc[: , :-1]] # vocabularies in training data\n",
    "        vocab_size = len(vocabularies) # vocabulary size\n",
    "        prior_category = {} # to store the prior probability of any category\n",
    "        posterior_category = {} # to store the posterior information for each category\n",
    "        for category in self.freq.keys():\n",
    "            for word in vocabularies:\n",
    "                count_specific_word_in_category_case = outcomesCategoryPD[category][word].sum() + 1 # count the specific word in the specific category\n",
    "                prob_word_given_category[category][word] = np.log(count_specific_word_in_category_case / (number_of_word_in_category_case[category]+vocab_size)) # calculate the conditional probability using Laplace Smooting\n",
    "                prior_category[category] = np.log(prob_category[category]) # store the prior probability for each category\n",
    "                posterior_category[category] = prob_word_given_category[category][word] + prior_category[category]\n",
    "        return prior_category, prob_word_given_category,vocabularies,outcomesCategoryPD # return the prior category, conditional probabilities, vocabularies\n",
    "    \n",
    "    def predict(self, X): # X is X_test -> outcomesTestPD\n",
    "        test_views = []\n",
    "        for test_index in X.index:\n",
    "            test_features = X.loc[test_index]\n",
    "            test_features = test_features[test_features > 0]\n",
    "            test_view = [feature for feature in test_features.to_frame().T.columns]\n",
    "            test_views.append(test_view)\n",
    "        test_views\n",
    "        print(len(test_views[0]))\n",
    "        y_pred = np.array([]).astype(np.object)\n",
    "        for view in test_views:\n",
    "            posterior_for_category = {}\n",
    "            for category in self.freq.keys():\n",
    "                posterior_for_category[category] = prior_category[category]\n",
    "                for token in view:\n",
    "                    if(token in vocabularies):\n",
    "                        posterior_for_category[category] += prob_word_given_category[category][token] # sum all the log likelihood with the prior probability \n",
    "            max_key = max(posterior_for_category, key=posterior_for_category.get) # return argmax of the predicted class\n",
    "            y_pred = np.append(y_pred, max_key) # append the predicted class to y_pred \n",
    "        return y_pred # return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_true, y_pred):\n",
    "    accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_clf_for_categories = NaiveBayesClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'books': 0.16608960235022557, 'camera': 0.16692896862868534, 'dvd': 0.16913230510964222, 'health': 0.16703388941349281, 'music': 0.1685027804007974, 'software': 0.16231245409715664}\n"
     ]
    }
   ],
   "source": [
    "prior_category, prob_word_given_category,vocabularies,outcomesCategoryPD = nb_clf_for_categories.fit(X_train,y_train,clf_label+\"_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174\n"
     ]
    }
   ],
   "source": [
    "y_pred = nb_clf_for_categories.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = calculate_accuracy(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9068401174989509\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(word='the'|label = neg) = -3.039581590486498\n",
      "P(word='of'|label = neg) = -3.6407692653538737\n",
      "P(word='and'|label = neg) = -3.6599966934410935\n",
      "P(word='to'|label = neg) = -3.7279279937030863\n",
      "P(word='is'|label = neg) = -4.138712263288851\n",
      "P(word='in'|label = neg) = -4.21415445845571\n",
      "P(word='this'|label = neg) = -4.377251500081034\n",
      "P(word='it'|label = neg) = -4.385181306573279\n",
      "P(word='book'|label = neg) = -4.451598190103922\n",
      "P(word='that'|label = neg) = -4.483082272037205\n"
     ]
    }
   ],
   "source": [
    "#prob_word_given_category[\"neg\"][\"like\"], prob_word_given_category[\"pos\"][\"like\"]\n",
    "#prob_word_given_category[\"neg\"][\"book\"], prob_word_given_category[\"pos\"][\"book\"]\n",
    "cond_neg = dict(sorted(prob_word_given_category[\"books\"].items(), key=lambda item: item[1], reverse=True))\n",
    "count = 1\n",
    "for key in cond_neg:\n",
    "    print(\"P(word='{}'|{}) = {}\".format(key,\"label = neg\",cond_neg[key]))\n",
    "    if(count == 10): break\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(word='the'|label = pos) = -3.039581590486498\n",
      "P(word='of'|label = pos) = -3.6407692653538737\n",
      "P(word='and'|label = pos) = -3.6599966934410935\n",
      "P(word='to'|label = pos) = -3.7279279937030863\n",
      "P(word='is'|label = pos) = -4.138712263288851\n",
      "P(word='in'|label = pos) = -4.21415445845571\n",
      "P(word='this'|label = pos) = -4.377251500081034\n",
      "P(word='it'|label = pos) = -4.385181306573279\n",
      "P(word='book'|label = pos) = -4.451598190103922\n",
      "P(word='that'|label = pos) = -4.483082272037205\n"
     ]
    }
   ],
   "source": [
    "cond_pos = dict(sorted(prob_word_given_category[\"books\"].items(), key=lambda item: item[1], reverse=True))\n",
    "count = 1\n",
    "for key in cond_pos:\n",
    "    print(\"P(word='{}'|{}) = {}\".format(key,\"label = pos\",cond_pos[key]))\n",
    "    if(count == 10): break\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printConfusionMatrix(y_pred, y_test):\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[362   1  35   3   3  13]\n",
      " [  0 395   0   5   1   7]\n",
      " [ 14  10 332   4  24   4]\n",
      " [  1  34   0 354   1  18]\n",
      " [  0   3  15   0 371   5]\n",
      " [  4   6   5   2   4 347]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       books       0.95      0.87      0.91       417\n",
      "      camera       0.88      0.97      0.92       408\n",
      "         dvd       0.86      0.86      0.86       388\n",
      "      health       0.96      0.87      0.91       408\n",
      "       music       0.92      0.94      0.93       394\n",
      "    software       0.88      0.94      0.91       368\n",
      "\n",
      "    accuracy                           0.91      2383\n",
      "   macro avg       0.91      0.91      0.91      2383\n",
      "weighted avg       0.91      0.91      0.91      2383\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "printConfusionMatrix(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['books', 'dvd', 'health', ..., 'health', 'dvd', 'software'],\n",
       "       dtype=object),\n",
       " array(['dvd', 'music', 'books', ..., 'books', 'health', 'books'],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train,y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Accuracy With The Sentiment Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_clf_for_categories = NaiveBayesClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.49554086664568253, 'pos': 0.5044591333543175}\n"
     ]
    }
   ],
   "source": [
    "prior_category, prob_word_given_category,vocabularies,outcomesCategoryPD = nb_clf_for_categories.fit(X_train,y_train,clf_label+\"_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262\n"
     ]
    }
   ],
   "source": [
    "y_pred = nb_clf_for_categories.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = calculate_accuracy(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.819555182543013\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printConfusionMatrix(y_pred, y_test):\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1027  164]\n",
      " [ 266  926]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.79      0.86      0.83      1191\n",
      "         pos       0.85      0.78      0.81      1192\n",
      "\n",
      "    accuracy                           0.82      2383\n",
      "   macro avg       0.82      0.82      0.82      2383\n",
      "weighted avg       0.82      0.82      0.82      2383\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "printConfusionMatrix(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train,y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF (using TfidfVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "vectorizer = TfidfVectorizer(stop_words=ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectorizer.fit_transform(X_train) \n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names()\n",
    "print(len(feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train.toarray(), columns=feature_names)\n",
    "X_test = pd.DataFrame(X_test.toarray(), columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[clf_label+\"_label\"] = np.array(y_train, dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train, dtype='object')\n",
    "y_test = np.array(y_test, dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayesClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_category, prob_word_given_category,vocabularies,outcomesCategoryPD = nb.fit(X_train,y_train,clf_label+\"_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = calculate_accuracy(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.819555182543013\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_strong_words_in_positive = dict(sorted(prob_word_given_category[\"pos\"].items(), key=lambda item: item[1], reverse=True))\n",
    "most_strong_words_in_negative = dict(sorted(prob_word_given_category[\"neg\"].items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(word='the'|label = neg) = -2.986205259442668\n",
      "P(word='to'|label = neg) = -3.6395180852701943\n",
      "P(word='and'|label = neg) = -3.674097760937825\n",
      "P(word='of'|label = neg) = -3.889243467489403\n",
      "P(word='it'|label = neg) = -3.929729803339705\n",
      "P(word='is'|label = neg) = -4.104418698339079\n",
      "P(word='this'|label = neg) = -4.183943112599831\n",
      "P(word='that'|label = neg) = -4.380076703002806\n",
      "P(word='in'|label = neg) = -4.405225771423476\n",
      "P(word='for'|label = neg) = -4.652111317653339\n"
     ]
    }
   ],
   "source": [
    "count = 1\n",
    "for key in most_strong_words_in_negative:\n",
    "    print(\"P(word='{}'|{}) = {}\".format(key,\"label = neg\",most_strong_words_in_negative[key]))\n",
    "    if(count == 10): break\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(word='the'|label = pos) = -2.948732535743196\n",
      "P(word='and'|label = pos) = -3.530831159849995\n",
      "P(word='to'|label = pos) = -3.697052407444448\n",
      "P(word='of'|label = pos) = -3.8130752113016464\n",
      "P(word='is'|label = pos) = -3.987906518225581\n",
      "P(word='it'|label = pos) = -3.990856497186509\n",
      "P(word='this'|label = pos) = -4.232056922719085\n",
      "P(word='in'|label = pos) = -4.300068018340222\n",
      "P(word='that'|label = pos) = -4.515308201100143\n",
      "P(word='for'|label = pos) = -4.564112626637668\n"
     ]
    }
   ],
   "source": [
    "count = 1\n",
    "for key in most_strong_words_in_positive:\n",
    "    print(\"P(word='{}'|{}) = {}\".format(key,\"label = pos\",most_strong_words_in_positive[key]))\n",
    "    if(count == 10): break\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printConfusionMatrix(y_pred, y_test):\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1027  164]\n",
      " [ 266  926]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.79      0.86      0.83      1191\n",
      "         pos       0.85      0.78      0.81      1192\n",
      "\n",
      "    accuracy                           0.82      2383\n",
      "   macro avg       0.82      0.82      0.82      2383\n",
      "weighted avg       0.82      0.82      0.82      2383\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "printConfusionMatrix(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF (using CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.feature_extraction.text  import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words=ENGLISH_STOP_WORDS)  #instantiate CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_vector = vectorizer.fit_transform(X_train) # this steps generates word counts for the words in the documets \n",
    "#X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer()"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Compute the IDF values\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) \n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print idf values \n",
    "df_idf = pd.DataFrame(tfidf_transformer.idf_, index=vectorizer.get_feature_names(),columns=[\"idf_weights\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idf_weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>2.283118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>just</th>\n",
       "      <td>2.303383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>2.485473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great</th>\n",
       "      <td>2.506073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>2.568532</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       idf_weights\n",
       "like      2.283118\n",
       "just      2.303383\n",
       "good      2.485473\n",
       "great     2.506073\n",
       "time      2.568532"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort ascending \n",
    "df_idf.sort_values(by=['idf_weights']).head() #Notice that the words ‘just’ and ‘like’ have the lowest IDF values. This is expected as these words appear in each and every document in our collection. The lower the IDF value of a word, the less unique it is to any particular document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Compute the TFIDF score for the documents\n",
    "X_train=vectorizer.transform(X_train) \n",
    " \n",
    "# tf-idf scores \n",
    "tf_idf_vector=tfidf_transformer.transform(X_train) # to compute the tf-idf scores for documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<9531x41571 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 461832 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names()  # get feature names\n",
    "first_document_vector=tf_idf_vector[0] #get tfidf vector for first document \n",
    "shape = first_document_vector.shape\n",
    "tf_idf_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>onions</th>\n",
       "      <td>0.761914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cut</th>\n",
       "      <td>0.214762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hahhahaha</th>\n",
       "      <td>0.205523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>onion</th>\n",
       "      <td>0.190478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chopping</th>\n",
       "      <td>0.185635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tfidf\n",
       "onions     0.761914\n",
       "cut        0.214762\n",
       "hahhahaha  0.205523\n",
       "onion      0.190478\n",
       "chopping   0.185635"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print the scores \n",
    "df = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"]) \n",
    "df.sort_values(by=[\"tfidf\"],ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['onions', 'cut', 'hahhahaha', 'onion', 'chopping', 'carrot', 'coz', 'vegetables', 'bought', 'tears', 'dont', 'broke', 'soft', 'suggest', 'mentioned', '15', 'save', 'buying', 'days', 'probably', 'buy', 'great', 'good'])\n"
     ]
    }
   ],
   "source": [
    "sorted_vals = df.sort_values(by=[\"tfidf\"],ascending=False)\n",
    "most_weighted_words = {}\n",
    "for index in sorted_vals.index:\n",
    "    features = sorted_vals.loc[index]\n",
    "    if(features[\"tfidf\"] > 0):\n",
    "        most_weighted_words[features.name] = features[\"tfidf\"]\n",
    "print(most_weighted_words.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train.toarray(), columns=feature_names) #convert series to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[clf_label+\"_label\"] = np.array(y_train, dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train, dtype='object')\n",
    "y_test = np.array(y_test, dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0003</th>\n",
       "      <th>000mb</th>\n",
       "      <th>004144</th>\n",
       "      <th>007</th>\n",
       "      <th>00am</th>\n",
       "      <th>00pm</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>...</th>\n",
       "      <th>½kerlund</th>\n",
       "      <th>½lan</th>\n",
       "      <th>½sistance</th>\n",
       "      <th>½ttbullar</th>\n",
       "      <th>½ve</th>\n",
       "      <th>áron</th>\n",
       "      <th>árpád</th>\n",
       "      <th>ça</th>\n",
       "      <th>único</th>\n",
       "      <th>sentiment_category_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41572 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  0003  000mb  004144  007  00am  00pm  01  02  ...  ½kerlund  ½lan  \\\n",
       "0   0    0     0      0       0    0     0     0   0   0  ...         0     0   \n",
       "1   0    0     0      0       0    0     0     0   0   0  ...         0     0   \n",
       "2   0    0     0      0       0    0     0     0   0   0  ...         0     0   \n",
       "3   0    0     0      0       0    0     0     0   0   0  ...         0     0   \n",
       "4   0    0     0      0       0    0     0     0   0   0  ...         0     0   \n",
       "\n",
       "   ½sistance  ½ttbullar  ½ve  áron  árpád  ça  único  sentiment_category_label  \n",
       "0          0          0    0     0      0   0      0                       neg  \n",
       "1          0          0    0     0      0   0      0                       pos  \n",
       "2          0          0    0     0      0   0      0                       neg  \n",
       "3          0          0    0     0      0   0      0                       pos  \n",
       "4          0          0    0     0      0   0      0                       neg  \n",
       "\n",
       "[5 rows x 41572 columns]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = NaiveBayesClassifier()\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_category, prob_word_given_category,vocabularies,outcomesCategoryPD = nb.fit(X_train,y_train,clf_label+\"_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.DataFrame(X_test.toarray(), columns=feature_names)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = calculate_accuracy(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8196810742761225\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words=ENGLISH_STOP_WORDS,ngram_range=(2, 2))\n",
    "X_train = vectorizer.fit_transform(X_train) \n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "428145"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names()\n",
    "len(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train.toarray(), columns=feature_names)\n",
    "X_test = pd.DataFrame(X_test.toarray(), columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.iloc[:,:42031]\n",
    "X_train[clf_label+\"_label\"] = np.array(y_train, dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.iloc[:,:42031]\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train, dtype='object')\n",
    "y_test = np.array(y_test, dtype='object')\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_clf_for_categories = NaiveBayesClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_category, prob_word_given_category,vocabularies,outcomesCategoryPD = nb_clf_for_categories.fit(X_train,y_train,clf_label+\"_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nb_clf_for_categories.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = calculate_accuracy(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printConfusionMatrix(y_pred, y_test):\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "printConfusionMatrix(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_pos = dict(sorted(prob_word_given_category[\"pos\"].items(), key=lambda item: item[1], reverse=True))\n",
    "count = 1\n",
    "for key in cond_pos:\n",
    "    print(\"P(word='{}'|{}) = {}\".format(key,\"label = pos\",cond_pos[key]))\n",
    "    if(count == 10): break\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_neg = dict(sorted(prob_word_given_category[\"neg\"].items(), key=lambda item: item[1], reverse=True))\n",
    "count = 1\n",
    "for key in cond_neg:\n",
    "    print(\"P(word='{}'|{}) = {}\".format(key,\"label = neg\",cond_neg[key]))\n",
    "    if(count == 10): break\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
